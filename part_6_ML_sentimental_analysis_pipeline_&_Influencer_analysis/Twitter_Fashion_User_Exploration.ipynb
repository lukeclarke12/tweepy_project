{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "colab": {
      "name": "Twitter_Fashion_User_Exploration.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "r7KFWdiNX2CZ",
        "34umvKULX2Cf"
      ],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "BCOUSZgbX2CD"
      },
      "source": [
        "# variables that contain the user credentials to access the twitter api\n",
        "\n",
        "\n",
        "api_key = \"xxxx\"                                 #consumer key\n",
        "api_key_secret = \"xxxx\" #consumer key secret\n",
        "bearer_token = \"xxxx\"\n",
        "access_token = \"xxxx\"\n",
        "access_token_secret = \"xxxx\""
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L3isdvsqX2CE"
      },
      "source": [
        "import tweepy\n",
        "from tweepy import API \n",
        "from tweepy import Cursor\n",
        "from tweepy.streaming import StreamListener\n",
        "from tweepy import OAuthHandler\n",
        "from tweepy import Stream\n",
        "import pandas as pd\n",
        "import numpy as np \n",
        "import matplotlib.pyplot as plt\n",
        "from textblob import TextBlob\n",
        "import re                           \n",
        "import tensorflow.compat.v2 as tf\n",
        "import tensorflow_datasets as tfds"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "57GqjAC2X2CF"
      },
      "source": [
        "# # # # TWITTER CLIENT # # # #\n",
        "class TwitterClient():\n",
        "    def __init__(self, twitter_user=None):\n",
        "        self.auth = TwitterAuthenticator().authenticate_twitter_app()\n",
        "        self.twitter_client = API(self.auth)\n",
        "\n",
        "        self.twitter_user = twitter_user\n",
        "        \n",
        "        \n",
        "    def get_twitter_client_API(self):                   # Adding a new function here to allow us to easily return the twitter client\n",
        "        return self.twitter_client\n",
        "    \n",
        "\n",
        "    def get_user_timeline_tweets(self, num_tweets):\n",
        "        tweets = []\n",
        "        for tweet in Cursor(self.twitter_client.user_timeline, id=self.twitter_user).items(num_tweets):\n",
        "            tweets.append(tweet)\n",
        "        return tweets\n",
        "\n",
        "    def get_friend_list(self, num_friends):\n",
        "        friend_list = []\n",
        "        for friend in Cursor(self.twitter_client.friends, id=self.twitter_user).items(num_friends):\n",
        "            friend_list.append(friend)\n",
        "        return friend_list\n",
        "\n",
        "    def get_home_timeline_tweets(self, num_tweets):\n",
        "        home_timeline_tweets = []\n",
        "        for tweet in Cursor(self.twitter_client.home_timeline, id=self.twitter_user).items(num_tweets):\n",
        "            home_timeline_tweets.append(tweet)\n",
        "        return home_timeline_tweets\n",
        "\n",
        "\n",
        "# # # # TWITTER AUTHENTICATER # # # #\n",
        "class TwitterAuthenticator():\n",
        "\n",
        "    def authenticate_twitter_app(self):\n",
        "        auth = OAuthHandler(api_key, api_key_secret)\n",
        "        auth.set_access_token(access_token, access_token_secret)\n",
        "        return auth\n",
        "\n",
        "# # # # TWITTER STREAMER # # # #\n",
        "class TwitterStreamer():\n",
        "    \"\"\"\n",
        "    Class for streaming and processing live tweets.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        self.twitter_autenticator = TwitterAuthenticator()    \n",
        "\n",
        "    def stream_tweets(self, fetched_tweets_filename, hash_tag_list):\n",
        "        # This handles Twitter authetification and the connection to Twitter Streaming API\n",
        "        listener = TwitterListener(fetched_tweets_filename)\n",
        "        auth = self.twitter_autenticator.authenticate_twitter_app() \n",
        "        stream = Stream(auth, listener)\n",
        "\n",
        "        # This line filter Twitter Streams to capture data by the keywords: \n",
        "        stream.filter(track=hash_tag_list)\n",
        "\n",
        "\n",
        "# # # # TWITTER STREAM LISTENER # # # #\n",
        "class TwitterListener(StreamListener):\n",
        "    \"\"\"\n",
        "    This is a basic listener that just prints received tweets to stdout.\n",
        "    \"\"\"\n",
        "    def __init__(self, fetched_tweets_filename):\n",
        "        self.fetched_tweets_filename = fetched_tweets_filename\n",
        "\n",
        "    def on_data(self, data):\n",
        "        try:\n",
        "            print(data)\n",
        "            with open(self.fetched_tweets_filename, 'a') as tf:\n",
        "                tf.write(data)\n",
        "            return True\n",
        "        except BaseException as e:\n",
        "            print(\"Error on_data %s\" % str(e))\n",
        "        return True\n",
        "          \n",
        "    def on_error(self, status):\n",
        "        if status == 420:\n",
        "            # Returning False on_data method in case rate limit occurs.\n",
        "            return False\n",
        "        print(status)\n",
        "\n",
        "\n",
        "        \n",
        "\n",
        "class TweetAnalyzer():\n",
        "    \"\"\"\n",
        "    Functionatlity for analyzing and categorizing content from tweets\n",
        "    \"\"\"\n",
        "    \n",
        "    def clean_tweet(self, tweet):\n",
        "        return ' '.join(re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\", \" \", tweet).split())\n",
        "        # This function we will use to clean a particular tweet passed to it\n",
        "        # This just removes special characters and hyperlinks etc.\n",
        "        \n",
        "        \n",
        "    def analyze_sentiment(self, tweet):\n",
        "        analysis = TextBlob(self.clean_tweet(tweet))\n",
        "        # This function we will use to run a sentiment analysis on a tweet using text_blob\n",
        "        \n",
        "        if analysis.sentiment.polarity > 0:                    # function to tell us the polarity of the tweet is positive or negative\n",
        "            return 1                                           # Positive Tweet\n",
        "        \n",
        "        elif analysis.sentiment.polarity == 0:                 # Neutral Tweet\n",
        "            return 0\n",
        "            \n",
        "        else:                                                  # Negative Tweet\n",
        "            return -1\n",
        "    \n",
        "    \n",
        "    def tweets_to_dataframe(self, tweets):\n",
        "        \n",
        "        \n",
        "        \n",
        "        df = pd.DataFrame(data=[tweet.created_at for tweet in tweets], columns=['Date'])  # passing tweet text to a Dataframe\n",
        "        \n",
        "        # I should really create to dataframes with a common column for Tweet and User here to avoid creating redundant data\n",
        "        df['Tweet Text'] = np.array([tweet.text for tweet in tweets])\n",
        "        df['Tweet Length'] = np.array([len(tweet.text) for tweet in tweets])\n",
        "        df['Tweet ID'] = np.array([tweet.id for tweet in tweets])\n",
        "        df['Favorite Count'] = np.array([tweet.favorite_count for tweet in tweets])\n",
        "        df['Retweet Count'] = np.array([tweet.retweet_count for tweet in tweets])\n",
        "        df['User Name'] = np.array([tweet.user.name for tweet in tweets])\n",
        "        df['User ID'] = np.array([tweet.user.id for tweet in tweets])\n",
        "        df['User Location'] = np.array([tweet.user.location for tweet in tweets])\n",
        "        df['User Follower Count'] = np.array([tweet.user.followers_count for tweet in tweets])\n",
        "        \n",
        "        \n",
        "        \n",
        "        return df\n",
        "        \n",
        "        \n"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-PS_Zh13X2CV"
      },
      "source": [
        "if __name__ == '__main__':\n",
        " \n",
        "    twitter_client = TwitterClient()                                                  # specifying a twitter client object\n",
        "    tweet_analyzer = TweetAnalyzer()                                                  # specifying a tweet analyzer object\n",
        "    api = twitter_client.get_twitter_client_API()\n",
        "    \n",
        "    \n",
        "    "
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_z-M_8rFX2CH",
        "outputId": "8fa90f09-e7fc-4c2d-b408-87af139d1909"
      },
      "source": [
        "# Getting friends of lukeclarke21\n",
        "# Returns a default of 20 and up to a max of 200 per page\n",
        "\n",
        "screen_name = 'lukeclarke21'\n",
        "luke_clarke_friends = set()\n",
        "\n",
        "for friend in api.friends(screen_name, count=200): \n",
        "    luke_clarke_friends.add(friend.screen_name)\n",
        "    \n",
        "\n",
        "luke_clarke_friends\n",
        "\n"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'GiGiHadid',\n",
              " 'KendallJenner',\n",
              " 'KimKardashian',\n",
              " 'KylieJenner',\n",
              " 'khloekardashian',\n",
              " 'kourtneykardash'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0AfNNjRgX2CK"
      },
      "source": [
        "# # # I set up a qtrt\n",
        "\n",
        "tweets = []\n",
        "\n",
        "for friend in luke_clarke_friends:\n",
        "        tweets.append(api.user_timeline(screen_name=friend, count=8))\n",
        "        "
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QjAvm1S3X2CU",
        "outputId": "c6fb6e07-2299-4f5d-d5c4-26891787849d"
      },
      "source": [
        "len(tweets)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pac61odTX2CU",
        "outputId": "b5498ad3-fd7c-4da3-d40b-95ca760504bf"
      },
      "source": [
        "# # # My tweets list was returning a list with lists of 2 tweets above so I need to flatten it \n",
        "\n",
        "flat_tweet_list = []\n",
        "for sublist in tweets:\n",
        "    for item in sublist:\n",
        "        flat_tweet_list.append(item)\n",
        "        \n",
        "len(flat_tweet_list)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "48"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 302
        },
        "id": "hDvjedD7X2CV",
        "outputId": "0dc7513b-2862-4b52-9e40-422295a4b3e9"
      },
      "source": [
        "\n",
        "df = tweet_analyzer.tweets_to_dataframe(flat_tweet_list)                                     # Creating a dataframe from the tweet_analyzer object, that we pass to the tweets_to_dataframe function in the tweet analyzer class\n",
        "df['Sentiment'] = np.array([tweet_analyzer.analyze_sentiment(tweet) for tweet in df['Tweet Text']])\n",
        "df.head()"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Date</th>\n",
              "      <th>Tweet Text</th>\n",
              "      <th>Tweet Length</th>\n",
              "      <th>Tweet ID</th>\n",
              "      <th>Favorite Count</th>\n",
              "      <th>Retweet Count</th>\n",
              "      <th>User Name</th>\n",
              "      <th>User ID</th>\n",
              "      <th>User Location</th>\n",
              "      <th>User Follower Count</th>\n",
              "      <th>Sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2021-02-03 04:52:03</td>\n",
              "      <td>That’s all 💋🧚🏼</td>\n",
              "      <td>14</td>\n",
              "      <td>1356827770978848769</td>\n",
              "      <td>8325</td>\n",
              "      <td>244</td>\n",
              "      <td>Khloé</td>\n",
              "      <td>32959253</td>\n",
              "      <td></td>\n",
              "      <td>29032145</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2021-02-03 04:51:53</td>\n",
              "      <td>🧚🏼🧚🏼🧚🏼🧚🏼🧚🏼🧚🏼🧚🏼🧚🏼🧚🏼</td>\n",
              "      <td>18</td>\n",
              "      <td>1356827726187876354</td>\n",
              "      <td>6169</td>\n",
              "      <td>280</td>\n",
              "      <td>Khloé</td>\n",
              "      <td>32959253</td>\n",
              "      <td></td>\n",
              "      <td>29032145</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2021-02-02 06:24:01</td>\n",
              "      <td>I love you guys! I am off to dreamland 🧚🏼✨🌙🧚🏼</td>\n",
              "      <td>45</td>\n",
              "      <td>1356488526502658048</td>\n",
              "      <td>8602</td>\n",
              "      <td>225</td>\n",
              "      <td>Khloé</td>\n",
              "      <td>32959253</td>\n",
              "      <td></td>\n",
              "      <td>29032145</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2021-02-02 06:22:14</td>\n",
              "      <td>@kyliesbirkins @kourtneykardash She won’t</td>\n",
              "      <td>41</td>\n",
              "      <td>1356488075598127106</td>\n",
              "      <td>206</td>\n",
              "      <td>5</td>\n",
              "      <td>Khloé</td>\n",
              "      <td>32959253</td>\n",
              "      <td></td>\n",
              "      <td>29032145</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2021-02-02 06:20:33</td>\n",
              "      <td>@kourtneykardash Wait you’re here</td>\n",
              "      <td>33</td>\n",
              "      <td>1356487652317425664</td>\n",
              "      <td>1392</td>\n",
              "      <td>16</td>\n",
              "      <td>Khloé</td>\n",
              "      <td>32959253</td>\n",
              "      <td></td>\n",
              "      <td>29032145</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                 Date  ... Sentiment\n",
              "0 2021-02-03 04:52:03  ...         0\n",
              "1 2021-02-03 04:51:53  ...         0\n",
              "2 2021-02-02 06:24:01  ...         1\n",
              "3 2021-02-02 06:22:14  ...         0\n",
              "4 2021-02-02 06:20:33  ...         0\n",
              "\n",
              "[5 rows x 11 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "szAyx8kMX2CV"
      },
      "source": [
        "# Now Lets Import Spacy To Analyze Tweet Text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TxNxvPXxX2CW"
      },
      "source": [
        "import spacy\n",
        "import string\n",
        "from spacy.lang.en import English\n",
        "from collections import Counter\n",
        "from spacy.lang.en.stop_words import STOP_WORDS"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x1k9GAH0X2CW"
      },
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "O3SsNYVBX2CW",
        "outputId": "8d7ebe36-9014-4a0a-afaa-f252821f4cc7"
      },
      "source": [
        "\"\"\" This could also have been used to create a new DataFrame column with spacy nlp applied to the corresponding Tweet Text in each row\"\"\"\n",
        "\n",
        "#df['NLP Text'] = df['Twitter Text'].apply(lambda x: nlp(x))"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "' This could also have been used to create a new DataFrame column with spacy nlp applied to the corresponding Tweet Text in each row'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ce_oq41KX2CW"
      },
      "source": [
        "**Here, I am putting every all of the tweet text from every tweet in the dataframe into one string so I can analyze all the tweets together with spacy!**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "id": "BFz0m7ZOX2CX",
        "outputId": "faa31ad7-0af5-46b0-b99b-5cf870699d08"
      },
      "source": [
        "df['Tweet Text Split'] = df['Tweet Text'].apply(lambda x : str(x).split(\".\"))\n",
        "\n",
        "# get string of all sentences\n",
        "super_tweet_string = \" \".join([\" \".join(row) for row in df[\"Tweet Text Split\"]])\n",
        "\n",
        "super_tweet_string"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"That’s all 💋\\U0001f9da🏼 \\U0001f9da🏼\\U0001f9da🏼\\U0001f9da🏼\\U0001f9da🏼\\U0001f9da🏼\\U0001f9da🏼\\U0001f9da🏼\\U0001f9da🏼\\U0001f9da🏼 I love you guys! I am off to dreamland \\U0001f9da🏼✨🌙\\U0001f9da🏼 @kyliesbirkins @kourtneykardash She won’t @kourtneykardash Wait you’re here @offthetabIe I don’t know  Are you tired or are you not @NarbehKardash You’re welcome @traevonceyah Don’t believe it for a second! Not with that face  💣 two pretty best friends https://t co/e9yXsrfmbQ launching today at 9am!! my new limited edition 8 Piece Mini Set features my best-selling Kylie Skin essentials, av… https://t co/MSbOWrbidI my Valentine’s Day shop is officially open! 💗 https://t co/h7emRJXnP8 everything &amp; more https://t co/j8S3Razck5 RT @chipswithpizza: Yay! My @KylieJenner @kyliecosmetics lip oil has arrived! Smells like coconut and the doe foot applicator is amazing \\U0001f929… a love story https://t co/QERk28b5GG that’s my best friend https://t co/1T7Hmy2rj5 happy friday ✨ https://t co/89BTVDUFxc https://t co/5ms7AsFCFJ RT @kkwbeauty: The wait is almost over! Announcement coming soon 🕑 #KKWBEAUTY https://t co/FmIyl2m8BV I hope you have a great day ✨ https://t co/IRCj79oKMs https://t co/n9egbstAzw https://t co/Ej7laz0UAT https://t co/fLSWEL6gI6 COMING FEB 5: @SKIMS SILK  Introducing a rich collection of romantic loungewear in golden tones and sensual styles,… https://t co/bIUaEd6OV8 JUST RESTOCKED: @SKIMS COZY! Your favorite loungewear is back in stock! Don’t miss out on our fan favorite sets tha… https://t co/gvicVvS2vf Happy Birthday Stormi!!!! OMG Stormi ⛈ You are the smartest, sweetest, silliest girl I know! I can’t believe you ar… https://t co/G2PRtV1y3O 🌧🌧🌧 \\U0001f90d🙏🏼!!! https://t co/78uzvEuDEn a new day! a good day! RT @JoeBiden: It’s a new day in America  RT @Versace: #KendallJenner 🔱 Preview the fierce faces of the #VersaceSS21 campaign and our new #VersaceLaMedusa handbag \\n\\nChief Creative O… sweet-sour https://t co/kVt8FNu825 \\U0001f970\\U0001f90d https://t co/imJ11PjJfd Secret to my smile\\xa0@moonoralcare #moon_partner                                   https://t co/6Ke6vA4Yes https://t co/GjRaUogFII With all the favorite-things goodies I’m sending to friends who are about to be parents, the mail man must think I’… https://t co/8QILUQSBSz @gigimyfeels Wash your hair as little as possible, obviously this is remnants of pregnancy hair / no hot tools (bc… https://t co/UnhBDySCyK @GHOSTOFSARA I have not forgotten  Got u Wanna get this painted for our house \\U0001f97a❤️ https://t co/3UdXSQhTip @bryanboy love RT @MSNBC: WATCH: Amanda Gorman, the youngest inaugural poet in U S  history, recites a poem on unity in the U S  at #Inauguration2021 http… @GigiHadidFan Also arugula salads !!!\\n\\nBut during fashion month in Europe it was hard to find bagels so I ate mostl… https://t co/9d3XCjp24E @GigiHadidFan It went it waves  Everything bagels/ extra cream cheese \\U0001f96f triple chunk brownies 🍫 for breakfast- sour… https://t co/PqmvGlIMj9 so good 🖤 https://t co/dSdPzz40CY Check out the items our Poosh team is loving this month\\nhttps://t co/3U1u9t4uoG Can't get enough of this chic beverage \\nhttps://t co/mn52CxNg2Q Yes please \\U0001f90d https://t co/d4EhbXoAOT Adding to my cart ASAP\\nhttps://t co/6HrDsT6pp4 Tips for feeling more joy, love, and wonder \\nhttps://t co/RnP0FiKfSb Fun new items added to my #KardashianKloset! Shop my collection here: https://t co/wUAivDmhfk It's time to update your winter wardrobe \\nhttps://t co/eFMb85s5q1\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T6m-58qjX2CX"
      },
      "source": [
        "fashion_tweets = nlp(super_tweet_string)"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qllFhM-SX2CX",
        "outputId": "ccf33cab-b11b-43f0-81aa-2f605ae8fa06"
      },
      "source": [
        "# Lets count the 5 most common words to see if it's a product type or brand name \n",
        "\n",
        "# Create our list of punctuation marks\n",
        "punctuations = string.punctuation\n",
        "\n",
        "char_counter = Counter()\n",
        "\n",
        "for token in fashion_tweets:\n",
        "    if token.pos_ == 'NOUN' and token.text not in STOP_WORDS and token.text not in punctuations and token.text != 'co' and token.text != 'https://t' and token.text != 'amp' and token.text != 'http://t' and token.text != '💕':\n",
        "        char_counter[token.text] += 1\n",
        "\n",
        "        \n",
        "char_counter.most_common(35)"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('🏼', 9),\n",
              " ('day', 4),\n",
              " ('✨', 3),\n",
              " ('friends', 2),\n",
              " ('love', 2),\n",
              " ('collection', 2),\n",
              " ('loungewear', 2),\n",
              " ('\\U0001f90d', 2),\n",
              " ('hair', 2),\n",
              " ('month', 2),\n",
              " ('items', 2),\n",
              " ('guys', 1),\n",
              " ('@kyliesbirkins', 1),\n",
              " ('second', 1),\n",
              " ('face', 1),\n",
              " ('today', 1),\n",
              " ('edition', 1),\n",
              " ('Piece', 1),\n",
              " ('Mini', 1),\n",
              " ('Set', 1),\n",
              " ('essentials', 1),\n",
              " ('av', 1),\n",
              " ('shop', 1),\n",
              " ('lip', 1),\n",
              " ('oil', 1),\n",
              " ('applicator', 1),\n",
              " ('\\U0001f929', 1),\n",
              " ('story', 1),\n",
              " ('friend', 1),\n",
              " ('@kkwbeauty', 1),\n",
              " ('wait', 1),\n",
              " ('Announcement', 1),\n",
              " ('KKWBEAUTY', 1),\n",
              " ('FmIyl2m8BV', 1),\n",
              " ('n9egbstAzw', 1)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2OngcwfbX2CY"
      },
      "source": [
        "The only relevant words I can see above are **'leather'** and **'jacket'**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kkVm0oSNX2CY",
        "outputId": "7d147dad-6fae-4c01-805c-78924be7633a"
      },
      "source": [
        "# Top 20 most common lemmas\n",
        "\n",
        "lemma_counter = Counter()\n",
        "\n",
        "for token in fashion_tweets:\n",
        "  if token.pos_ != \"PROPN\" and not token.is_punct and not token.is_digit and not token.is_space and token.text not in STOP_WORDS and token.text not in punctuations and token.text != 'co' and token.text != 'https://t' and token.text != 'amp' and token.text != 'http://t' and token.text != '💕':\n",
        "    lemma_counter[token.lemma_] += 1\n",
        "\n",
        "lemma_counter.most_common(10)\n"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('-PRON-', 18),\n",
              " ('🏼', 13),\n",
              " ('\\U0001f9da', 10),\n",
              " ('new', 5),\n",
              " ('love', 4),\n",
              " ('good', 4),\n",
              " ('day', 4),\n",
              " ('✨', 3),\n",
              " ('friend', 3),\n",
              " ('favorite', 3)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r7KFWdiNX2CZ"
      },
      "source": [
        "# Now, I want to build a machine learning model to analyze the sentiment of tweets\n",
        "\n",
        "This is a dataset with 162,980 unique tweets where the sentiment of each tweet is labelled -1 (Negative), 0 (Neutral) and 1 (Positive)\n",
        "\n",
        "Dataset: https://www.kaggle.com/cosmos98/twitter-and-reddit-sentimental-analysis-dataset?select=Twitter_Data.csv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "emiCG-8zX2CZ"
      },
      "source": [
        "# Step 1: Generating the Dataset (First Part Of ETL: Extract, Transform & Load)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 198
        },
        "id": "DxNOVT76X2Ca",
        "outputId": "c244f292-6a55-44cd-be36-31b3fa979dbd"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Reading the dataset with no columns titles and with latin encoding \n",
        "df_tweets = pd.read_csv(\"twitter_sentiment.csv\")\n",
        "\n",
        "df_tweets.head()"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>clean_text</th>\n",
              "      <th>category</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>when modi promised “minimum government maximum...</td>\n",
              "      <td>-1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>talk all the nonsense and continue all the dra...</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>what did just say vote for modi  welcome bjp t...</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>asking his supporters prefix chowkidar their n...</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>answer who among these the most powerful world...</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                          clean_text  category\n",
              "0  when modi promised “minimum government maximum...      -1.0\n",
              "1  talk all the nonsense and continue all the dra...       0.0\n",
              "2  what did just say vote for modi  welcome bjp t...       1.0\n",
              "3  asking his supporters prefix chowkidar their n...       1.0\n",
              "4  answer who among these the most powerful world...       1.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 407
        },
        "id": "0KSS6IH2X2Cc",
        "outputId": "b04c272c-6343-4a00-c755-4e035fd88fc2"
      },
      "source": [
        "df_tweets = df_tweets.rename(columns={'clean_text':'tweets', 'category':'sentiment'})\n",
        "df_tweets"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tweets</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>when modi promised “minimum government maximum...</td>\n",
              "      <td>-1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>talk all the nonsense and continue all the dra...</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>what did just say vote for modi  welcome bjp t...</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>asking his supporters prefix chowkidar their n...</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>answer who among these the most powerful world...</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>57848</th>\n",
              "      <td>and clarify you ardent karyakarta for trs when...</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>57849</th>\n",
              "      <td>during before announced feat scientists today ...</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>57850</th>\n",
              "      <td>why would act helps modi</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>57851</th>\n",
              "      <td>are with modi and modi with and you are useles...</td>\n",
              "      <td>-1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>57852</th>\n",
              "      <td>you and your</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>57853 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                  tweets  sentiment\n",
              "0      when modi promised “minimum government maximum...       -1.0\n",
              "1      talk all the nonsense and continue all the dra...        0.0\n",
              "2      what did just say vote for modi  welcome bjp t...        1.0\n",
              "3      asking his supporters prefix chowkidar their n...        1.0\n",
              "4      answer who among these the most powerful world...        1.0\n",
              "...                                                  ...        ...\n",
              "57848  and clarify you ardent karyakarta for trs when...        0.0\n",
              "57849  during before announced feat scientists today ...        1.0\n",
              "57850                          why would act helps modi         0.0\n",
              "57851  are with modi and modi with and you are useles...       -1.0\n",
              "57852                                       you and your        NaN\n",
              "\n",
              "[57853 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KayQA4OnX2Cc",
        "outputId": "00ca57ef-612c-480a-b5bd-97af6bf89bf7"
      },
      "source": [
        "df_tweets.shape"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(57853, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AWStsl_AX2Cc",
        "outputId": "d0a3df9c-43dd-4665-a699-4e91bc518a81"
      },
      "source": [
        "# overall has the most non-null values which equate to the shape of our df_review dataframe\n",
        "\n",
        "df_tweets.info"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<bound method DataFrame.info of                                                   tweets  sentiment\n",
              "0      when modi promised “minimum government maximum...       -1.0\n",
              "1      talk all the nonsense and continue all the dra...        0.0\n",
              "2      what did just say vote for modi  welcome bjp t...        1.0\n",
              "3      asking his supporters prefix chowkidar their n...        1.0\n",
              "4      answer who among these the most powerful world...        1.0\n",
              "...                                                  ...        ...\n",
              "57848  and clarify you ardent karyakarta for trs when...        0.0\n",
              "57849  during before announced feat scientists today ...        1.0\n",
              "57850                          why would act helps modi         0.0\n",
              "57851  are with modi and modi with and you are useles...       -1.0\n",
              "57852                                       you and your        NaN\n",
              "\n",
              "[57853 rows x 2 columns]>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "37FrQZxQX2Cd"
      },
      "source": [
        "# Step 2: Cleaning The Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ccx0f_qZX2Cd",
        "outputId": "5f2da869-0b35-47be-fb6b-c1b761846eb3"
      },
      "source": [
        "df_tweets.isnull().sum()"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tweets       1\n",
              "sentiment    1\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lpc9GfNcX2Cd"
      },
      "source": [
        "df_tweets = df.dropna()"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dXNUMXvFX2Ce",
        "outputId": "5c9b72f5-753b-4904-c279-dc186290080f"
      },
      "source": [
        "df_tweets.isnull().sum()"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Date                   0\n",
              "Tweet Text             0\n",
              "Tweet Length           0\n",
              "Tweet ID               0\n",
              "Favorite Count         0\n",
              "Retweet Count          0\n",
              "User Name              0\n",
              "User ID                0\n",
              "User Location          0\n",
              "User Follower Count    0\n",
              "Sentiment              0\n",
              "Tweet Text Split       0\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "749PFYatX2Ce"
      },
      "source": [
        "# Step 3: Tokenize The Data\n",
        "\n",
        "For text data and especially for sentiment analysis we want tokenize it, in order to draw relationships and similarities and deal with it further, as we have seen we can so with tokens.\n",
        "\n",
        "Also, Further Cleaning of what we do not need. E.g. STOP_WORDS, Punctuations etc."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UzYuyO3wX2Ce"
      },
      "source": [
        "import spacy\n",
        "import string                                          # function where we can access all the puntuations in the english language\n",
        "from spacy.lang.en.stop_words import STOP_WORDS\n",
        "from spacy.lang.en import English\n",
        "\n",
        "\n",
        "# create a list of punctuations\n",
        "punctuations = string.punctuation\n",
        "\n",
        "\n",
        "# Load english tokenizer, tagger, parser, NER, and word vectors\n",
        "# A function that automatically tokenizes what we pass to it\n",
        "parser = English()\n",
        "\n",
        "\n",
        "# creating our tokenizer function\n",
        "\n",
        "def spacy_tokenizer(sentence):\n",
        "    my_tokens = parser(sentence)              \n",
        "    \n",
        "    # now we have all our tokens, but we need to remove all the unnecessary info e.g. stop words, punctuations\n",
        "    \n",
        "    \n",
        "    # Lemmatizing each token and converting each token into lowercase\n",
        "    my_tokens = [word.lemma_.lower().strip() if word.lemma_ != \"-PRON-\" else word.lower_ for word in my_tokens]\n",
        "    \n",
        "    # Removing stop words and punctuations\n",
        "    my_tokens = [word for word in my_tokens if word not in STOP_WORDS and word not in punctuations]\n",
        "    \n",
        "    return my_tokens"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CKQzgnupX2Ce"
      },
      "source": [
        "# Step 4: Encode/Standardize dataset\n",
        "\n",
        "Once we have all of our tokens identified we need to encode and standized the dataset. In other words we need to find ways to give meaning to the words by identifying words and drawing relationships and similarities from one word to others.\n",
        "\n",
        "This will then allow us to draw conclusions and make classifications later on in this machine learning project."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YRLr8zdCX2Cf"
      },
      "source": [
        "from sklearn.base import TransformerMixin\n",
        "\n",
        "# This function will clean any text I pass to it \n",
        "\n",
        "def clean_text(text):\n",
        "     return text.strip().lower()\n",
        "    \n",
        "    \n",
        "#Custom transformer using Python standard library (you could use spacy as well)\n",
        "class predictors(TransformerMixin):\n",
        "\n",
        "    def transform(self, X, **transform_params):\n",
        "        return [clean_text(text) for text in X]\n",
        "\n",
        "    def fit(self, X, y=None, **fit_params):\n",
        "        return self\n",
        "\n",
        "    def get_params(self, deep=True):\n",
        "        return {}"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "34umvKULX2Cf"
      },
      "source": [
        "# TF-IDF\n",
        "\n",
        "From the lectures we learned the TF-IDF (Term-Frequency - Inverse Document Frequency is a function that calculates a numerical statistic which is a way of calculating how important a word is to a document, a collection of documents of a corpus of text.\n",
        "\n",
        "The tf–idf value increases proportionally to the number of times a word appears in the document and is offset by the number of documents in the corpus that contain the word, which helps to adjust for the fact that some words appear more frequently in general."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZeciYv69X2Cf"
      },
      "source": [
        "# TF-IDF Vectorizer\n",
        "\n",
        "Converts a collection of raw documents to a matrix of TF-IDF features. So, in our case, we will be able to pass \"my_tokens\" from each review to the TF-IDF vectorizer which will extract features from the text, which we can then user to later teach our model how to classify the text as (1, 2, 3, 4, 5) or (Very Bad, Poor, Good, Very Good, Excellent)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wYilFXQNX2Cf"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Using tf_idf\n",
        "tfvectorizer = TfidfVectorizer(tokenizer = spacy_tokenizer)\n",
        "\n"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N8pnJXReX2Cg"
      },
      "source": [
        "\n",
        "# Train-Test split\n",
        "In machine learning, we always need to split our datasets into train and test. We will use one for training the model and another one to check how the model performs. Luckily, sklearn comes with an in-built function for this.\n",
        "\n",
        "The split is done randomly, but we can attribute a seed value to make it stable for developing purposes. The usually split is 20% test and 80% train."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "nXHi8qYSX2Cg",
        "outputId": "85225842-8bd6-4925-c3cf-d72dbd0d5e18"
      },
      "source": [
        "df_tweets"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Date</th>\n",
              "      <th>Tweet Text</th>\n",
              "      <th>Tweet Length</th>\n",
              "      <th>Tweet ID</th>\n",
              "      <th>Favorite Count</th>\n",
              "      <th>Retweet Count</th>\n",
              "      <th>User Name</th>\n",
              "      <th>User ID</th>\n",
              "      <th>User Location</th>\n",
              "      <th>User Follower Count</th>\n",
              "      <th>Sentiment</th>\n",
              "      <th>Tweet Text Split</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2021-02-03 04:52:03</td>\n",
              "      <td>That’s all 💋🧚🏼</td>\n",
              "      <td>14</td>\n",
              "      <td>1356827770978848769</td>\n",
              "      <td>8325</td>\n",
              "      <td>244</td>\n",
              "      <td>Khloé</td>\n",
              "      <td>32959253</td>\n",
              "      <td></td>\n",
              "      <td>29032145</td>\n",
              "      <td>0</td>\n",
              "      <td>[That’s all 💋🧚🏼]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2021-02-03 04:51:53</td>\n",
              "      <td>🧚🏼🧚🏼🧚🏼🧚🏼🧚🏼🧚🏼🧚🏼🧚🏼🧚🏼</td>\n",
              "      <td>18</td>\n",
              "      <td>1356827726187876354</td>\n",
              "      <td>6169</td>\n",
              "      <td>280</td>\n",
              "      <td>Khloé</td>\n",
              "      <td>32959253</td>\n",
              "      <td></td>\n",
              "      <td>29032145</td>\n",
              "      <td>0</td>\n",
              "      <td>[🧚🏼🧚🏼🧚🏼🧚🏼🧚🏼🧚🏼🧚🏼🧚🏼🧚🏼]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2021-02-02 06:24:01</td>\n",
              "      <td>I love you guys! I am off to dreamland 🧚🏼✨🌙🧚🏼</td>\n",
              "      <td>45</td>\n",
              "      <td>1356488526502658048</td>\n",
              "      <td>8602</td>\n",
              "      <td>225</td>\n",
              "      <td>Khloé</td>\n",
              "      <td>32959253</td>\n",
              "      <td></td>\n",
              "      <td>29032145</td>\n",
              "      <td>1</td>\n",
              "      <td>[I love you guys! I am off to dreamland 🧚🏼✨🌙🧚🏼]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2021-02-02 06:22:14</td>\n",
              "      <td>@kyliesbirkins @kourtneykardash She won’t</td>\n",
              "      <td>41</td>\n",
              "      <td>1356488075598127106</td>\n",
              "      <td>206</td>\n",
              "      <td>5</td>\n",
              "      <td>Khloé</td>\n",
              "      <td>32959253</td>\n",
              "      <td></td>\n",
              "      <td>29032145</td>\n",
              "      <td>0</td>\n",
              "      <td>[@kyliesbirkins @kourtneykardash She won’t]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2021-02-02 06:20:33</td>\n",
              "      <td>@kourtneykardash Wait you’re here</td>\n",
              "      <td>33</td>\n",
              "      <td>1356487652317425664</td>\n",
              "      <td>1392</td>\n",
              "      <td>16</td>\n",
              "      <td>Khloé</td>\n",
              "      <td>32959253</td>\n",
              "      <td></td>\n",
              "      <td>29032145</td>\n",
              "      <td>0</td>\n",
              "      <td>[@kourtneykardash Wait you’re here]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>2021-02-02 06:19:31</td>\n",
              "      <td>@offthetabIe I don’t know. Are you tired or ar...</td>\n",
              "      <td>55</td>\n",
              "      <td>1356487394950668288</td>\n",
              "      <td>32</td>\n",
              "      <td>0</td>\n",
              "      <td>Khloé</td>\n",
              "      <td>32959253</td>\n",
              "      <td></td>\n",
              "      <td>29032145</td>\n",
              "      <td>-1</td>\n",
              "      <td>[@offthetabIe I don’t know,  Are you tired or ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>2021-02-02 06:19:08</td>\n",
              "      <td>@NarbehKardash You’re welcome</td>\n",
              "      <td>29</td>\n",
              "      <td>1356487295717703682</td>\n",
              "      <td>62</td>\n",
              "      <td>4</td>\n",
              "      <td>Khloé</td>\n",
              "      <td>32959253</td>\n",
              "      <td></td>\n",
              "      <td>29032145</td>\n",
              "      <td>1</td>\n",
              "      <td>[@NarbehKardash You’re welcome]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>2021-02-02 06:16:49</td>\n",
              "      <td>@traevonceyah Don’t believe it for a second! N...</td>\n",
              "      <td>66</td>\n",
              "      <td>1356486714240364545</td>\n",
              "      <td>102</td>\n",
              "      <td>2</td>\n",
              "      <td>Khloé</td>\n",
              "      <td>32959253</td>\n",
              "      <td></td>\n",
              "      <td>29032145</td>\n",
              "      <td>0</td>\n",
              "      <td>[@traevonceyah Don’t believe it for a second! ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>2021-02-03 20:33:56</td>\n",
              "      <td>two pretty best friends https://t.co/e9yXsrfmbQ</td>\n",
              "      <td>47</td>\n",
              "      <td>1357064801587847168</td>\n",
              "      <td>201434</td>\n",
              "      <td>16994</td>\n",
              "      <td>Kylie Jenner</td>\n",
              "      <td>236699098</td>\n",
              "      <td></td>\n",
              "      <td>37299394</td>\n",
              "      <td>1</td>\n",
              "      <td>[two pretty best friends https://t, co/e9yXsrf...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>2021-02-02 16:23:20</td>\n",
              "      <td>launching today at 9am!! my new limited editio...</td>\n",
              "      <td>140</td>\n",
              "      <td>1356639349970882560</td>\n",
              "      <td>8719</td>\n",
              "      <td>272</td>\n",
              "      <td>Kylie Jenner</td>\n",
              "      <td>236699098</td>\n",
              "      <td></td>\n",
              "      <td>37299394</td>\n",
              "      <td>1</td>\n",
              "      <td>[launching today at 9am!! my new limited editi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>2021-01-28 20:54:01</td>\n",
              "      <td>my Valentine’s Day shop is officially open! 💗 ...</td>\n",
              "      <td>69</td>\n",
              "      <td>1354895529440755714</td>\n",
              "      <td>7316</td>\n",
              "      <td>232</td>\n",
              "      <td>Kylie Jenner</td>\n",
              "      <td>236699098</td>\n",
              "      <td></td>\n",
              "      <td>37299394</td>\n",
              "      <td>0</td>\n",
              "      <td>[my Valentine’s Day shop is officially open! 💗...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>2021-01-27 00:27:12</td>\n",
              "      <td>everything &amp;amp; more https://t.co/j8S3Razck5</td>\n",
              "      <td>45</td>\n",
              "      <td>1354224402838282252</td>\n",
              "      <td>261391</td>\n",
              "      <td>16661</td>\n",
              "      <td>Kylie Jenner</td>\n",
              "      <td>236699098</td>\n",
              "      <td></td>\n",
              "      <td>37299394</td>\n",
              "      <td>1</td>\n",
              "      <td>[everything &amp;amp; more https://t, co/j8S3Razck5]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>2021-01-26 18:00:11</td>\n",
              "      <td>RT @chipswithpizza: Yay! My @KylieJenner @kyli...</td>\n",
              "      <td>139</td>\n",
              "      <td>1354127007404101633</td>\n",
              "      <td>0</td>\n",
              "      <td>94</td>\n",
              "      <td>Kylie Jenner</td>\n",
              "      <td>236699098</td>\n",
              "      <td></td>\n",
              "      <td>37299394</td>\n",
              "      <td>1</td>\n",
              "      <td>[RT @chipswithpizza: Yay! My @KylieJenner @kyl...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>2021-01-23 20:19:31</td>\n",
              "      <td>a love story https://t.co/QERk28b5GG</td>\n",
              "      <td>36</td>\n",
              "      <td>1353074908096393228</td>\n",
              "      <td>126997</td>\n",
              "      <td>6072</td>\n",
              "      <td>Kylie Jenner</td>\n",
              "      <td>236699098</td>\n",
              "      <td></td>\n",
              "      <td>37299394</td>\n",
              "      <td>1</td>\n",
              "      <td>[a love story https://t, co/QERk28b5GG]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>2021-01-22 20:13:43</td>\n",
              "      <td>that’s my best friend https://t.co/1T7Hmy2rj5</td>\n",
              "      <td>45</td>\n",
              "      <td>1352711058780110848</td>\n",
              "      <td>213206</td>\n",
              "      <td>9156</td>\n",
              "      <td>Kylie Jenner</td>\n",
              "      <td>236699098</td>\n",
              "      <td></td>\n",
              "      <td>37299394</td>\n",
              "      <td>1</td>\n",
              "      <td>[that’s my best friend https://t, co/1T7Hmy2rj5]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>2021-01-22 20:12:48</td>\n",
              "      <td>happy friday ✨ https://t.co/89BTVDUFxc https:/...</td>\n",
              "      <td>62</td>\n",
              "      <td>1352710829846630400</td>\n",
              "      <td>18380</td>\n",
              "      <td>366</td>\n",
              "      <td>Kylie Jenner</td>\n",
              "      <td>236699098</td>\n",
              "      <td></td>\n",
              "      <td>37299394</td>\n",
              "      <td>1</td>\n",
              "      <td>[happy friday ✨ https://t, co/89BTVDUFxc https...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>2021-02-04 02:22:47</td>\n",
              "      <td>RT @kkwbeauty: The wait is almost over! Announ...</td>\n",
              "      <td>101</td>\n",
              "      <td>1357152594129428486</td>\n",
              "      <td>0</td>\n",
              "      <td>52</td>\n",
              "      <td>Kim Kardashian West</td>\n",
              "      <td>25365536</td>\n",
              "      <td></td>\n",
              "      <td>68843529</td>\n",
              "      <td>0</td>\n",
              "      <td>[RT @kkwbeauty: The wait is almost over! Annou...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>2021-02-03 15:36:39</td>\n",
              "      <td>I hope you have a great day ✨ https://t.co/IRC...</td>\n",
              "      <td>53</td>\n",
              "      <td>1356989986432634885</td>\n",
              "      <td>124254</td>\n",
              "      <td>5699</td>\n",
              "      <td>Kim Kardashian West</td>\n",
              "      <td>25365536</td>\n",
              "      <td></td>\n",
              "      <td>68843529</td>\n",
              "      <td>1</td>\n",
              "      <td>[I hope you have a great day ✨ https://t, co/I...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>2021-02-02 17:21:23</td>\n",
              "      <td>https://t.co/n9egbstAzw</td>\n",
              "      <td>23</td>\n",
              "      <td>1356653958991933448</td>\n",
              "      <td>12187</td>\n",
              "      <td>519</td>\n",
              "      <td>Kim Kardashian West</td>\n",
              "      <td>25365536</td>\n",
              "      <td></td>\n",
              "      <td>68843529</td>\n",
              "      <td>0</td>\n",
              "      <td>[https://t, co/n9egbstAzw]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>2021-02-02 17:21:21</td>\n",
              "      <td>https://t.co/Ej7laz0UAT</td>\n",
              "      <td>23</td>\n",
              "      <td>1356653947759517698</td>\n",
              "      <td>10685</td>\n",
              "      <td>454</td>\n",
              "      <td>Kim Kardashian West</td>\n",
              "      <td>25365536</td>\n",
              "      <td></td>\n",
              "      <td>68843529</td>\n",
              "      <td>0</td>\n",
              "      <td>[https://t, co/Ej7laz0UAT]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>2021-02-02 17:21:18</td>\n",
              "      <td>https://t.co/fLSWEL6gI6</td>\n",
              "      <td>23</td>\n",
              "      <td>1356653935445102601</td>\n",
              "      <td>1438</td>\n",
              "      <td>89</td>\n",
              "      <td>Kim Kardashian West</td>\n",
              "      <td>25365536</td>\n",
              "      <td></td>\n",
              "      <td>68843529</td>\n",
              "      <td>0</td>\n",
              "      <td>[https://t, co/fLSWEL6gI6]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>2021-02-02 17:21:14</td>\n",
              "      <td>COMING FEB 5: @SKIMS SILK. Introducing a rich ...</td>\n",
              "      <td>140</td>\n",
              "      <td>1356653921306112000</td>\n",
              "      <td>12396</td>\n",
              "      <td>440</td>\n",
              "      <td>Kim Kardashian West</td>\n",
              "      <td>25365536</td>\n",
              "      <td></td>\n",
              "      <td>68843529</td>\n",
              "      <td>1</td>\n",
              "      <td>[COMING FEB 5: @SKIMS SILK,  Introducing a ric...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>2021-02-01 17:00:41</td>\n",
              "      <td>JUST RESTOCKED: @SKIMS COZY! Your favorite lou...</td>\n",
              "      <td>140</td>\n",
              "      <td>1356286360148189190</td>\n",
              "      <td>7701</td>\n",
              "      <td>196</td>\n",
              "      <td>Kim Kardashian West</td>\n",
              "      <td>25365536</td>\n",
              "      <td></td>\n",
              "      <td>68843529</td>\n",
              "      <td>1</td>\n",
              "      <td>[JUST RESTOCKED: @SKIMS COZY! Your favorite lo...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>2021-02-01 15:30:37</td>\n",
              "      <td>Happy Birthday Stormi!!!! OMG Stormi ⛈ You are...</td>\n",
              "      <td>140</td>\n",
              "      <td>1356263696075419651</td>\n",
              "      <td>149140</td>\n",
              "      <td>7911</td>\n",
              "      <td>Kim Kardashian West</td>\n",
              "      <td>25365536</td>\n",
              "      <td></td>\n",
              "      <td>68843529</td>\n",
              "      <td>1</td>\n",
              "      <td>[Happy Birthday Stormi!!!! OMG Stormi ⛈ You ar...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>2021-01-24 01:49:13</td>\n",
              "      <td>🌧🌧🌧</td>\n",
              "      <td>3</td>\n",
              "      <td>1353157877943345152</td>\n",
              "      <td>63732</td>\n",
              "      <td>2602</td>\n",
              "      <td>Kendall</td>\n",
              "      <td>157140968</td>\n",
              "      <td>an airplane</td>\n",
              "      <td>30795531</td>\n",
              "      <td>0</td>\n",
              "      <td>[🌧🌧🌧]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>2021-01-21 15:36:13</td>\n",
              "      <td>🤍🙏🏼!!! https://t.co/78uzvEuDEn</td>\n",
              "      <td>30</td>\n",
              "      <td>1352278837984337921</td>\n",
              "      <td>14866</td>\n",
              "      <td>375</td>\n",
              "      <td>Kendall</td>\n",
              "      <td>157140968</td>\n",
              "      <td>an airplane</td>\n",
              "      <td>30795531</td>\n",
              "      <td>0</td>\n",
              "      <td>[🤍🙏🏼!!! https://t, co/78uzvEuDEn]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>2021-01-20 19:05:25</td>\n",
              "      <td>a new day! a good day!</td>\n",
              "      <td>22</td>\n",
              "      <td>1351969093994348544</td>\n",
              "      <td>110935</td>\n",
              "      <td>11850</td>\n",
              "      <td>Kendall</td>\n",
              "      <td>157140968</td>\n",
              "      <td>an airplane</td>\n",
              "      <td>30795531</td>\n",
              "      <td>1</td>\n",
              "      <td>[a new day! a good day!]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>2021-01-20 18:51:33</td>\n",
              "      <td>RT @JoeBiden: It’s a new day in America.</td>\n",
              "      <td>40</td>\n",
              "      <td>1351965606296186883</td>\n",
              "      <td>0</td>\n",
              "      <td>516503</td>\n",
              "      <td>Kendall</td>\n",
              "      <td>157140968</td>\n",
              "      <td>an airplane</td>\n",
              "      <td>30795531</td>\n",
              "      <td>1</td>\n",
              "      <td>[RT @JoeBiden: It’s a new day in America, ]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>2021-01-12 16:43:30</td>\n",
              "      <td>RT @Versace: #KendallJenner 🔱 Preview the fier...</td>\n",
              "      <td>140</td>\n",
              "      <td>1349034280471576577</td>\n",
              "      <td>0</td>\n",
              "      <td>910</td>\n",
              "      <td>Kendall</td>\n",
              "      <td>157140968</td>\n",
              "      <td>an airplane</td>\n",
              "      <td>30795531</td>\n",
              "      <td>1</td>\n",
              "      <td>[RT @Versace: #KendallJenner 🔱 Preview the fie...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>2021-01-08 17:37:22</td>\n",
              "      <td>sweet-sour https://t.co/kVt8FNu825</td>\n",
              "      <td>34</td>\n",
              "      <td>1347598283451977734</td>\n",
              "      <td>285983</td>\n",
              "      <td>33033</td>\n",
              "      <td>Kendall</td>\n",
              "      <td>157140968</td>\n",
              "      <td>an airplane</td>\n",
              "      <td>30795531</td>\n",
              "      <td>1</td>\n",
              "      <td>[sweet-sour https://t, co/kVt8FNu825]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>2020-12-28 21:10:02</td>\n",
              "      <td>🥰🤍 https://t.co/imJ11PjJfd</td>\n",
              "      <td>26</td>\n",
              "      <td>1343665537700691968</td>\n",
              "      <td>27170</td>\n",
              "      <td>1208</td>\n",
              "      <td>Kendall</td>\n",
              "      <td>157140968</td>\n",
              "      <td>an airplane</td>\n",
              "      <td>30795531</td>\n",
              "      <td>0</td>\n",
              "      <td>[🥰🤍 https://t, co/imJ11PjJfd]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>2020-12-15 17:57:50</td>\n",
              "      <td>Secret to my smile @moonoralcare #moon_partner...</td>\n",
              "      <td>128</td>\n",
              "      <td>1338906126897672192</td>\n",
              "      <td>20283</td>\n",
              "      <td>684</td>\n",
              "      <td>Kendall</td>\n",
              "      <td>157140968</td>\n",
              "      <td>an airplane</td>\n",
              "      <td>30795531</td>\n",
              "      <td>-1</td>\n",
              "      <td>[Secret to my smile @moonoralcare #moon_partne...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>2021-02-03 19:23:26</td>\n",
              "      <td>With all the favorite-things goodies I’m sendi...</td>\n",
              "      <td>140</td>\n",
              "      <td>1357047061464637443</td>\n",
              "      <td>96123</td>\n",
              "      <td>5505</td>\n",
              "      <td>Gigi Hadid</td>\n",
              "      <td>171682730</td>\n",
              "      <td></td>\n",
              "      <td>10107876</td>\n",
              "      <td>1</td>\n",
              "      <td>[With all the favorite-things goodies I’m send...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>2021-02-03 00:57:54</td>\n",
              "      <td>@gigimyfeels Wash your hair as little as possi...</td>\n",
              "      <td>139</td>\n",
              "      <td>1356768843142746115</td>\n",
              "      <td>6915</td>\n",
              "      <td>678</td>\n",
              "      <td>Gigi Hadid</td>\n",
              "      <td>171682730</td>\n",
              "      <td></td>\n",
              "      <td>10107876</td>\n",
              "      <td>-1</td>\n",
              "      <td>[@gigimyfeels Wash your hair as little as poss...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>2021-02-02 02:40:13</td>\n",
              "      <td>@GHOSTOFSARA I have not forgotten. Got u</td>\n",
              "      <td>40</td>\n",
              "      <td>1356432204847579140</td>\n",
              "      <td>25191</td>\n",
              "      <td>1505</td>\n",
              "      <td>Gigi Hadid</td>\n",
              "      <td>171682730</td>\n",
              "      <td></td>\n",
              "      <td>10107876</td>\n",
              "      <td>0</td>\n",
              "      <td>[@GHOSTOFSARA I have not forgotten,  Got u]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>2021-02-02 02:38:39</td>\n",
              "      <td>Wanna get this painted for our house 🥺❤️ https...</td>\n",
              "      <td>64</td>\n",
              "      <td>1356431809463074816</td>\n",
              "      <td>243494</td>\n",
              "      <td>18517</td>\n",
              "      <td>Gigi Hadid</td>\n",
              "      <td>171682730</td>\n",
              "      <td></td>\n",
              "      <td>10107876</td>\n",
              "      <td>0</td>\n",
              "      <td>[Wanna get this painted for our house 🥺❤️ http...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>2021-01-29 01:32:54</td>\n",
              "      <td>@bryanboy love</td>\n",
              "      <td>14</td>\n",
              "      <td>1354965712289353728</td>\n",
              "      <td>5431</td>\n",
              "      <td>540</td>\n",
              "      <td>Gigi Hadid</td>\n",
              "      <td>171682730</td>\n",
              "      <td></td>\n",
              "      <td>10107876</td>\n",
              "      <td>1</td>\n",
              "      <td>[@bryanboy love]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>2021-01-21 06:04:11</td>\n",
              "      <td>RT @MSNBC: WATCH: Amanda Gorman, the youngest ...</td>\n",
              "      <td>140</td>\n",
              "      <td>1352134880243703809</td>\n",
              "      <td>0</td>\n",
              "      <td>6208</td>\n",
              "      <td>Gigi Hadid</td>\n",
              "      <td>171682730</td>\n",
              "      <td></td>\n",
              "      <td>10107876</td>\n",
              "      <td>0</td>\n",
              "      <td>[RT @MSNBC: WATCH: Amanda Gorman, the youngest...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>2021-01-18 16:25:25</td>\n",
              "      <td>@GigiHadidFan Also arugula salads !!!\\n\\nBut d...</td>\n",
              "      <td>140</td>\n",
              "      <td>1351204055553609733</td>\n",
              "      <td>18150</td>\n",
              "      <td>1489</td>\n",
              "      <td>Gigi Hadid</td>\n",
              "      <td>171682730</td>\n",
              "      <td></td>\n",
              "      <td>10107876</td>\n",
              "      <td>-1</td>\n",
              "      <td>[@GigiHadidFan Also arugula salads !!!\\n\\nBut ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>2021-01-18 16:20:35</td>\n",
              "      <td>@GigiHadidFan It went it waves. Everything bag...</td>\n",
              "      <td>140</td>\n",
              "      <td>1351202839092224003</td>\n",
              "      <td>23191</td>\n",
              "      <td>1423</td>\n",
              "      <td>Gigi Hadid</td>\n",
              "      <td>171682730</td>\n",
              "      <td></td>\n",
              "      <td>10107876</td>\n",
              "      <td>-1</td>\n",
              "      <td>[@GigiHadidFan It went it waves,  Everything b...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>2021-02-03 20:39:10</td>\n",
              "      <td>so good 🖤 https://t.co/dSdPzz40CY</td>\n",
              "      <td>33</td>\n",
              "      <td>1357066120973352962</td>\n",
              "      <td>855</td>\n",
              "      <td>28</td>\n",
              "      <td>Kourtney Kardashian</td>\n",
              "      <td>23617610</td>\n",
              "      <td></td>\n",
              "      <td>25857784</td>\n",
              "      <td>1</td>\n",
              "      <td>[so good 🖤 https://t, co/dSdPzz40CY]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41</th>\n",
              "      <td>2021-02-03 17:23:01</td>\n",
              "      <td>Check out the items our Poosh team is loving t...</td>\n",
              "      <td>79</td>\n",
              "      <td>1357016754820579329</td>\n",
              "      <td>1210</td>\n",
              "      <td>33</td>\n",
              "      <td>Kourtney Kardashian</td>\n",
              "      <td>23617610</td>\n",
              "      <td></td>\n",
              "      <td>25857784</td>\n",
              "      <td>1</td>\n",
              "      <td>[Check out the items our Poosh team is loving ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42</th>\n",
              "      <td>2021-02-02 21:18:24</td>\n",
              "      <td>Can't get enough of this chic beverage.\\nhttps...</td>\n",
              "      <td>63</td>\n",
              "      <td>1356713604318339073</td>\n",
              "      <td>982</td>\n",
              "      <td>33</td>\n",
              "      <td>Kourtney Kardashian</td>\n",
              "      <td>23617610</td>\n",
              "      <td></td>\n",
              "      <td>25857784</td>\n",
              "      <td>0</td>\n",
              "      <td>[Can't get enough of this chic beverage, \\nhtt...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43</th>\n",
              "      <td>2021-02-02 06:19:02</td>\n",
              "      <td>Yes please 🤍 https://t.co/d4EhbXoAOT</td>\n",
              "      <td>36</td>\n",
              "      <td>1356487270778327041</td>\n",
              "      <td>3443</td>\n",
              "      <td>103</td>\n",
              "      <td>Kourtney Kardashian</td>\n",
              "      <td>23617610</td>\n",
              "      <td></td>\n",
              "      <td>25857784</td>\n",
              "      <td>0</td>\n",
              "      <td>[Yes please 🤍 https://t, co/d4EhbXoAOT]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44</th>\n",
              "      <td>2021-02-02 01:40:54</td>\n",
              "      <td>Adding to my cart ASAP\\nhttps://t.co/6HrDsT6pp4</td>\n",
              "      <td>46</td>\n",
              "      <td>1356417276388335617</td>\n",
              "      <td>1146</td>\n",
              "      <td>38</td>\n",
              "      <td>Kourtney Kardashian</td>\n",
              "      <td>23617610</td>\n",
              "      <td></td>\n",
              "      <td>25857784</td>\n",
              "      <td>0</td>\n",
              "      <td>[Adding to my cart ASAP\\nhttps://t, co/6HrDsT6...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45</th>\n",
              "      <td>2021-01-30 02:03:51</td>\n",
              "      <td>Tips for feeling more joy, love, and wonder.\\n...</td>\n",
              "      <td>68</td>\n",
              "      <td>1355335888251719680</td>\n",
              "      <td>2826</td>\n",
              "      <td>113</td>\n",
              "      <td>Kourtney Kardashian</td>\n",
              "      <td>23617610</td>\n",
              "      <td></td>\n",
              "      <td>25857784</td>\n",
              "      <td>1</td>\n",
              "      <td>[Tips for feeling more joy, love, and wonder, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>46</th>\n",
              "      <td>2021-01-29 21:09:07</td>\n",
              "      <td>Fun new items added to my #KardashianKloset! S...</td>\n",
              "      <td>93</td>\n",
              "      <td>1355261715642912768</td>\n",
              "      <td>1671</td>\n",
              "      <td>49</td>\n",
              "      <td>Kourtney Kardashian</td>\n",
              "      <td>23617610</td>\n",
              "      <td></td>\n",
              "      <td>25857784</td>\n",
              "      <td>1</td>\n",
              "      <td>[Fun new items added to my #KardashianKloset! ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47</th>\n",
              "      <td>2021-01-29 02:52:36</td>\n",
              "      <td>It's time to update your winter wardrobe.\\nhtt...</td>\n",
              "      <td>65</td>\n",
              "      <td>1354985768255541250</td>\n",
              "      <td>1155</td>\n",
              "      <td>38</td>\n",
              "      <td>Kourtney Kardashian</td>\n",
              "      <td>23617610</td>\n",
              "      <td></td>\n",
              "      <td>25857784</td>\n",
              "      <td>0</td>\n",
              "      <td>[It's time to update your winter wardrobe, \\nh...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                  Date  ...                                   Tweet Text Split\n",
              "0  2021-02-03 04:52:03  ...                                   [That’s all 💋🧚🏼]\n",
              "1  2021-02-03 04:51:53  ...                               [🧚🏼🧚🏼🧚🏼🧚🏼🧚🏼🧚🏼🧚🏼🧚🏼🧚🏼]\n",
              "2  2021-02-02 06:24:01  ...    [I love you guys! I am off to dreamland 🧚🏼✨🌙🧚🏼]\n",
              "3  2021-02-02 06:22:14  ...        [@kyliesbirkins @kourtneykardash She won’t]\n",
              "4  2021-02-02 06:20:33  ...                [@kourtneykardash Wait you’re here]\n",
              "5  2021-02-02 06:19:31  ...  [@offthetabIe I don’t know,  Are you tired or ...\n",
              "6  2021-02-02 06:19:08  ...                    [@NarbehKardash You’re welcome]\n",
              "7  2021-02-02 06:16:49  ...  [@traevonceyah Don’t believe it for a second! ...\n",
              "8  2021-02-03 20:33:56  ...  [two pretty best friends https://t, co/e9yXsrf...\n",
              "9  2021-02-02 16:23:20  ...  [launching today at 9am!! my new limited editi...\n",
              "10 2021-01-28 20:54:01  ...  [my Valentine’s Day shop is officially open! 💗...\n",
              "11 2021-01-27 00:27:12  ...   [everything &amp; more https://t, co/j8S3Razck5]\n",
              "12 2021-01-26 18:00:11  ...  [RT @chipswithpizza: Yay! My @KylieJenner @kyl...\n",
              "13 2021-01-23 20:19:31  ...            [a love story https://t, co/QERk28b5GG]\n",
              "14 2021-01-22 20:13:43  ...   [that’s my best friend https://t, co/1T7Hmy2rj5]\n",
              "15 2021-01-22 20:12:48  ...  [happy friday ✨ https://t, co/89BTVDUFxc https...\n",
              "16 2021-02-04 02:22:47  ...  [RT @kkwbeauty: The wait is almost over! Annou...\n",
              "17 2021-02-03 15:36:39  ...  [I hope you have a great day ✨ https://t, co/I...\n",
              "18 2021-02-02 17:21:23  ...                         [https://t, co/n9egbstAzw]\n",
              "19 2021-02-02 17:21:21  ...                         [https://t, co/Ej7laz0UAT]\n",
              "20 2021-02-02 17:21:18  ...                         [https://t, co/fLSWEL6gI6]\n",
              "21 2021-02-02 17:21:14  ...  [COMING FEB 5: @SKIMS SILK,  Introducing a ric...\n",
              "22 2021-02-01 17:00:41  ...  [JUST RESTOCKED: @SKIMS COZY! Your favorite lo...\n",
              "23 2021-02-01 15:30:37  ...  [Happy Birthday Stormi!!!! OMG Stormi ⛈ You ar...\n",
              "24 2021-01-24 01:49:13  ...                                              [🌧🌧🌧]\n",
              "25 2021-01-21 15:36:13  ...                  [🤍🙏🏼!!! https://t, co/78uzvEuDEn]\n",
              "26 2021-01-20 19:05:25  ...                           [a new day! a good day!]\n",
              "27 2021-01-20 18:51:33  ...        [RT @JoeBiden: It’s a new day in America, ]\n",
              "28 2021-01-12 16:43:30  ...  [RT @Versace: #KendallJenner 🔱 Preview the fie...\n",
              "29 2021-01-08 17:37:22  ...              [sweet-sour https://t, co/kVt8FNu825]\n",
              "30 2020-12-28 21:10:02  ...                      [🥰🤍 https://t, co/imJ11PjJfd]\n",
              "31 2020-12-15 17:57:50  ...  [Secret to my smile @moonoralcare #moon_partne...\n",
              "32 2021-02-03 19:23:26  ...  [With all the favorite-things goodies I’m send...\n",
              "33 2021-02-03 00:57:54  ...  [@gigimyfeels Wash your hair as little as poss...\n",
              "34 2021-02-02 02:40:13  ...        [@GHOSTOFSARA I have not forgotten,  Got u]\n",
              "35 2021-02-02 02:38:39  ...  [Wanna get this painted for our house 🥺❤️ http...\n",
              "36 2021-01-29 01:32:54  ...                                   [@bryanboy love]\n",
              "37 2021-01-21 06:04:11  ...  [RT @MSNBC: WATCH: Amanda Gorman, the youngest...\n",
              "38 2021-01-18 16:25:25  ...  [@GigiHadidFan Also arugula salads !!!\\n\\nBut ...\n",
              "39 2021-01-18 16:20:35  ...  [@GigiHadidFan It went it waves,  Everything b...\n",
              "40 2021-02-03 20:39:10  ...               [so good 🖤 https://t, co/dSdPzz40CY]\n",
              "41 2021-02-03 17:23:01  ...  [Check out the items our Poosh team is loving ...\n",
              "42 2021-02-02 21:18:24  ...  [Can't get enough of this chic beverage, \\nhtt...\n",
              "43 2021-02-02 06:19:02  ...            [Yes please 🤍 https://t, co/d4EhbXoAOT]\n",
              "44 2021-02-02 01:40:54  ...  [Adding to my cart ASAP\\nhttps://t, co/6HrDsT6...\n",
              "45 2021-01-30 02:03:51  ...  [Tips for feeling more joy, love, and wonder, ...\n",
              "46 2021-01-29 21:09:07  ...  [Fun new items added to my #KardashianKloset! ...\n",
              "47 2021-01-29 02:52:36  ...  [It's time to update your winter wardrobe, \\nh...\n",
              "\n",
              "[48 rows x 12 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j4goVgJDehAI",
        "outputId": "92b3d598-c88d-4df4-ff18-2af0bf37efec"
      },
      "source": [
        "df_tweets.info()"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 48 entries, 0 to 47\n",
            "Data columns (total 12 columns):\n",
            " #   Column               Non-Null Count  Dtype         \n",
            "---  ------               --------------  -----         \n",
            " 0   Date                 48 non-null     datetime64[ns]\n",
            " 1   Tweet Text           48 non-null     object        \n",
            " 2   Tweet Length         48 non-null     int64         \n",
            " 3   Tweet ID             48 non-null     int64         \n",
            " 4   Favorite Count       48 non-null     int64         \n",
            " 5   Retweet Count        48 non-null     int64         \n",
            " 6   User Name            48 non-null     object        \n",
            " 7   User ID              48 non-null     int64         \n",
            " 8   User Location        48 non-null     object        \n",
            " 9   User Follower Count  48 non-null     int64         \n",
            " 10  Sentiment            48 non-null     int64         \n",
            " 11  Tweet Text Split     48 non-null     object        \n",
            "dtypes: datetime64[ns](1), int64(7), object(4)\n",
            "memory usage: 4.9+ KB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HSXEsQ7GX2Cg"
      },
      "source": [
        "# Running with Optimal DataFrame\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Specifying our features & labels\n",
        "\n",
        "features = df_tweets[\"Tweet Text\"]\n",
        "labels = df_tweets[\"Sentiment\"]"
      ],
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A3k-TsYrX2Cg",
        "outputId": "b72b3fc3-5c1f-4835-83d6-32cf4158d558"
      },
      "source": [
        "features"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0                                        That’s all 💋🧚🏼\n",
              "1                                    🧚🏼🧚🏼🧚🏼🧚🏼🧚🏼🧚🏼🧚🏼🧚🏼🧚🏼\n",
              "2         I love you guys! I am off to dreamland 🧚🏼✨🌙🧚🏼\n",
              "3             @kyliesbirkins @kourtneykardash She won’t\n",
              "4                     @kourtneykardash Wait you’re here\n",
              "5     @offthetabIe I don’t know. Are you tired or ar...\n",
              "6                         @NarbehKardash You’re welcome\n",
              "7     @traevonceyah Don’t believe it for a second! N...\n",
              "8       two pretty best friends https://t.co/e9yXsrfmbQ\n",
              "9     launching today at 9am!! my new limited editio...\n",
              "10    my Valentine’s Day shop is officially open! 💗 ...\n",
              "11        everything &amp; more https://t.co/j8S3Razck5\n",
              "12    RT @chipswithpizza: Yay! My @KylieJenner @kyli...\n",
              "13                 a love story https://t.co/QERk28b5GG\n",
              "14        that’s my best friend https://t.co/1T7Hmy2rj5\n",
              "15    happy friday ✨ https://t.co/89BTVDUFxc https:/...\n",
              "16    RT @kkwbeauty: The wait is almost over! Announ...\n",
              "17    I hope you have a great day ✨ https://t.co/IRC...\n",
              "18                              https://t.co/n9egbstAzw\n",
              "19                              https://t.co/Ej7laz0UAT\n",
              "20                              https://t.co/fLSWEL6gI6\n",
              "21    COMING FEB 5: @SKIMS SILK. Introducing a rich ...\n",
              "22    JUST RESTOCKED: @SKIMS COZY! Your favorite lou...\n",
              "23    Happy Birthday Stormi!!!! OMG Stormi ⛈ You are...\n",
              "24                                                  🌧🌧🌧\n",
              "25                       🤍🙏🏼!!! https://t.co/78uzvEuDEn\n",
              "26                               a new day! a good day!\n",
              "27             RT @JoeBiden: It’s a new day in America.\n",
              "28    RT @Versace: #KendallJenner 🔱 Preview the fier...\n",
              "29                   sweet-sour https://t.co/kVt8FNu825\n",
              "30                           🥰🤍 https://t.co/imJ11PjJfd\n",
              "31    Secret to my smile @moonoralcare #moon_partner...\n",
              "32    With all the favorite-things goodies I’m sendi...\n",
              "33    @gigimyfeels Wash your hair as little as possi...\n",
              "34             @GHOSTOFSARA I have not forgotten. Got u\n",
              "35    Wanna get this painted for our house 🥺❤️ https...\n",
              "36                                       @bryanboy love\n",
              "37    RT @MSNBC: WATCH: Amanda Gorman, the youngest ...\n",
              "38    @GigiHadidFan Also arugula salads !!!\\n\\nBut d...\n",
              "39    @GigiHadidFan It went it waves. Everything bag...\n",
              "40                    so good 🖤 https://t.co/dSdPzz40CY\n",
              "41    Check out the items our Poosh team is loving t...\n",
              "42    Can't get enough of this chic beverage.\\nhttps...\n",
              "43                 Yes please 🤍 https://t.co/d4EhbXoAOT\n",
              "44      Adding to my cart ASAP\\nhttps://t.co/6HrDsT6pp4\n",
              "45    Tips for feeling more joy, love, and wonder.\\n...\n",
              "46    Fun new items added to my #KardashianKloset! S...\n",
              "47    It's time to update your winter wardrobe.\\nhtt...\n",
              "Name: Tweet Text, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dVrXZjZgX2Ch",
        "outputId": "dca069fe-394f-4940-844d-84959167ae8f"
      },
      "source": [
        "labels"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0     0\n",
              "1     0\n",
              "2     1\n",
              "3     0\n",
              "4     0\n",
              "5    -1\n",
              "6     1\n",
              "7     0\n",
              "8     1\n",
              "9     1\n",
              "10    0\n",
              "11    1\n",
              "12    1\n",
              "13    1\n",
              "14    1\n",
              "15    1\n",
              "16    0\n",
              "17    1\n",
              "18    0\n",
              "19    0\n",
              "20    0\n",
              "21    1\n",
              "22    1\n",
              "23    1\n",
              "24    0\n",
              "25    0\n",
              "26    1\n",
              "27    1\n",
              "28    1\n",
              "29    1\n",
              "30    0\n",
              "31   -1\n",
              "32    1\n",
              "33   -1\n",
              "34    0\n",
              "35    0\n",
              "36    1\n",
              "37    0\n",
              "38   -1\n",
              "39   -1\n",
              "40    1\n",
              "41    1\n",
              "42    0\n",
              "43    0\n",
              "44    0\n",
              "45    1\n",
              "46    1\n",
              "47    0\n",
              "Name: Sentiment, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aUypfq9UX2Ch"
      },
      "source": [
        "features_train, features_test, labels_train, labels_test = train_test_split(features, labels, test_size = 0.2, random_state=42)\n",
        "\n"
      ],
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9_QX-mU6X2Ch"
      },
      "source": [
        "print(\"Features Train Shape ==>\", features_train.shape)\n",
        "print(\"Features Test Shape ==>\", features_test.shape)\n",
        "print(\"Labels Train Shape ==>\",  labels_train.shape)\n",
        "print(\"Labels Test Shape ==>\", labels_test.shape)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9HM3OEZ5X2Ch"
      },
      "source": [
        "# The classifier\n",
        "\n",
        "With choosing a classifier, we are choosing the strategy for our model to learn. Since we are trying to do a classification (bad, neutral, good) we will need to choose algorithms that are classifiers.\n",
        "\n",
        "We can use Sklearns built in multi-layer perceptron classifier. In our case as we are trying to assign each review to a certain class i.e. (-1, 0, 1) or (bad, neutral, good)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4zxqoIcBX2Ci"
      },
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "# # Multi layer perceptron - Neural Network\n",
        "classifier_MLP = MLPClassifier(max_iter=50, hidden_layer_sizes = (100, 2), verbose = True)\n",
        "\n"
      ],
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lr1XRi0SX2Ci"
      },
      "source": [
        "# SVC classifier\n",
        "from sklearn.svm import LinearSVC\n",
        "\n",
        "classifier_SVC = LinearSVC(verbose=True)"
      ],
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GS82mo2XX2Ci"
      },
      "source": [
        "# Logistic Regression Classifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "classifier_LG = LogisticRegression(verbose=True)"
      ],
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uY0oe3yFX2Ci"
      },
      "source": [
        "# Step 5: Build A Pipeline\n",
        "\n",
        "Now we need to create an sklearn pipeline that:\n",
        "\n",
        "- Cleans and preprocess the text using our predictors class from above\n",
        "- Vectorizes the words with TF-IDF to create word matrixes from our text.\n",
        "- Load the MLP classifier in order to classifier the sentiment of each review."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NflQvKh-X2Ci"
      },
      "source": [
        "Pipeline Function:\n",
        "    \n",
        "A pipeline of transforms with a final estimator.\n",
        "\n",
        "Sequentially apply a list of transforms and a final estimator. Intermediate steps of the pipeline must be ‘transforms’, that is, they must implement fit and transform methods. The final estimator only needs to implement fit."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iI7Asu2tX2Ci"
      },
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# Create the pipeline to clean, tokenize, vectorize and classify\n",
        "\n",
        "\n",
        "                                                        # The train data passed through the pipeline is: \n",
        "pipe = Pipeline([(\"cleaner\", predictors()),             # Cleaned and tokenized as it is passed through the predictors class\n",
        "                 (\"vectorizer\", tfvectorizer),          # Then vectorized as it passes throught the tfvectorizer function we previously specified\n",
        "                 (\"classifier\", classifier_SVC)])       # Classified according to the MLP Classifier which we also previously specified\n",
        "\n",
        "\n",
        "                                                        # Note: The data passed to the Classifier must be previously vectorized\n",
        "                                                        #  and, The data passed to the vectorizer must be previously cleaned and tokenized\n",
        "                                                        # Thus, this method of this pipeline is very important\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2cNQ1EcFX2Cj",
        "outputId": "320bd711-eac7-4485-ed65-920e0cf28633"
      },
      "source": [
        "# Fitting our data \n",
        "# Watch the loss function decrease through each iteration as the neural network learns how to best adjust the weights and biases\n",
        "pipe.fit(features_train, labels_train)"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[LibLinear]"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(memory=None,\n",
              "         steps=[('cleaner', <__main__.predictors object at 0x7f911969a160>),\n",
              "                ('vectorizer',\n",
              "                 TfidfVectorizer(analyzer='word', binary=False,\n",
              "                                 decode_error='strict',\n",
              "                                 dtype=<class 'numpy.float64'>,\n",
              "                                 encoding='utf-8', input='content',\n",
              "                                 lowercase=True, max_df=1.0, max_features=None,\n",
              "                                 min_df=1, ngram_range=(1, 1), norm='l2',\n",
              "                                 preprocessor=None, smooth_idf=True,\n",
              "                                 stop_wor...\n",
              "                                 sublinear_tf=False,\n",
              "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
              "                                 tokenizer=<function spacy_tokenizer at 0x7f9165c321e0>,\n",
              "                                 use_idf=True, vocabulary=None)),\n",
              "                ('classifier',\n",
              "                 LinearSVC(C=1.0, class_weight=None, dual=True,\n",
              "                           fit_intercept=True, intercept_scaling=1,\n",
              "                           loss='squared_hinge', max_iter=1000,\n",
              "                           multi_class='ovr', penalty='l2', random_state=None,\n",
              "                           tol=0.0001, verbose=True))],\n",
              "         verbose=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PxNUftByX2Cj"
      },
      "source": [
        "# Now we pass our test_features to the model in order to make a predictions based on what it has learned in training\n",
        "# in order to predict the test_labels\n",
        "\n",
        "sample_prediction = pipe.predict(features_test)"
      ],
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xVkJdKLqX2Cj"
      },
      "source": [
        "for (sample, pred) in zip(features_test, sample_prediction):\n",
        "    print(sample, \"Prediction ==>\", pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JsDQ_6pqX2Cr"
      },
      "source": [
        "Now we can evaluate the model using different metrics, so that we can look at the three main performance metrics:\n",
        "\n",
        "**Accuracy:** Refers to the percentage of the total predictions our model makes that are completely correct.\n",
        "\n",
        "**Precision:** Describes the ratio of true positives to true positives plus false positives in our predictions.\n",
        "\n",
        "**Recall:** Describes the ratio of true positives to true positives plus false negatives in our predictions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0tzM6vQ2X2Cs"
      },
      "source": [
        "from sklearn import metrics\n",
        "\n",
        "# Model Accuracy\n",
        "\n",
        "# Remember the model never accuracy_score the test data which we hold back.\n",
        "# It is here where we can now compare the test data held back against the models predictions to measure its accuracy.\n",
        "# I can't use recall and precision here as I am not using binary classes\n",
        "\n",
        "print(\"Accuracy:\",metrics.accuracy_score(labels_test, sample_prediction))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R0516JJHX2Cs"
      },
      "source": [
        "# Confusion Matrix\n",
        "\n",
        "Based on the F1 - Score it appears that the model is good at predicting reviews that are 5-Stars, but then quite poor at predicting 1 star and 3 star reviews.\n",
        "\n",
        "Probably, because there are a lot more examples of 5 Stars reviews in our dataset and because it is easier to capture the sentiment for five star reviews based on the words people use is a review when making a 5 star review oe a 1, 2 or 3 star review. \n",
        "\n",
        "This is skewing the heatmap for the confusion matrix."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WTRwgAM5X2Cs"
      },
      "source": [
        "from sklearn import metrics\n",
        "\n",
        "print(\"Classification report for classifier %s:\\n%s\\n\"\n",
        "      % (classifier_MLP, metrics.classification_report(labels_test, sample_prediction)))\n",
        "print(\"Confusion matrix:\\n%s\" % metrics.confusion_matrix(labels_test, sample_prediction))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r0OinwJcX2Ct"
      },
      "source": [
        "\n",
        "import seaborn as sb\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "confusion_matrix = metrics.confusion_matrix(labels_test, sample_prediction                                        )\n",
        "sb.heatmap(confusion_matrix, cmap=plt.cm.inferno)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0dhCFoVHX2Ct"
      },
      "source": [
        "# Repeat: Using MLP Classifier\n",
        "\n",
        "I tried running running the above with the MLP classifier, however it was taking forever. So, now I will run the data through the MLP Classifier, however, I will use only a small portion of the initial data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MEiOQajoX2Ct"
      },
      "source": [
        "df_optimal = df_tweets[0:10000]\n",
        "df_optimal"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DrHzr6n9X2Cu"
      },
      "source": [
        "# Running with Optimal DataFrame\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Specifying our features & labels\n",
        "\n",
        "features = df_optimal[\"tweets\"]\n",
        "labels = df_optimal[\"sentiment\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VGnP7ljVX2Cu"
      },
      "source": [
        "features"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pr4GQy8YX2Cv"
      },
      "source": [
        "labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "98nd3FyVX2Cv"
      },
      "source": [
        "features_train, features_test, labels_train, labels_test = train_test_split(features, labels, test_size = 0.3, random_state=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ik8flb7yX2Cv"
      },
      "source": [
        "print(\"Features Train Shape ==>\", features_train.shape)\n",
        "print(\"Features Test Shape ==>\", features_test.shape)\n",
        "print(\"Labels Train Shape ==>\",  labels_train.shape)\n",
        "print(\"Labels Test Shape ==>\", labels_test.shape)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BTSO3BVfX2Cw"
      },
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# Create the pipeline to clean, tokenize, vectorize and classify\n",
        "\n",
        "\n",
        "                                                        # The train data passed through the pipeline is: \n",
        "pipe_2 = Pipeline([(\"cleaner\", predictors()),             # Cleaned and tokenized as it is passed through the predictors class\n",
        "                 (\"vectorizer\", tfvectorizer),          # Then vectorized as it passes throught the tfvectorizer function we previously specified\n",
        "                 (\"classifier\", classifier_MLP)])       # Classified according to the MLP Classifier which we also previously specified\n",
        "\n",
        "\n",
        "                                                        # Note: The data passed to the Classifier must be previously vectorized\n",
        "                                                        #  and, The data passed to the vectorizer must be previously cleaned and tokenized\n",
        "                                                        # Thus, this method of this pipeline is very important\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SSWQZyB3X2Cw"
      },
      "source": [
        "# Fitting our data \n",
        "# Watch the loss function decrease through each iteration as the neural network learns how to best adjust the weights and biases\n",
        "pipe_2.fit(features_train, labels_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fhyJb_9ZX2Cw"
      },
      "source": [
        "# Now we pass our test_features to the model in order to make a predictions based on what it has learned in training\n",
        "# in order to predict the test_labels\n",
        "\n",
        "sample_prediction = pipe_2.predict(features_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ayrwPPsnX2Cx"
      },
      "source": [
        "for (sample, pred) in zip(features_test, sample_prediction):\n",
        "    print(sample, \"Prediction ==>\", pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y1YQzoZGX2Cy"
      },
      "source": [
        "from sklearn import metrics\n",
        "\n",
        "# Model Accuracy\n",
        "\n",
        "print(\"Accuracy:\",metrics.accuracy_score(labels_test, sample_prediction))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8DZkzDosX2Cy"
      },
      "source": [
        "# Confusion Matrix\n",
        "\n",
        "from sklearn import metrics\n",
        "\n",
        "print(\"Classification report for classifier %s:\\n%s\\n\"\n",
        "      % (classifier_MLP, metrics.classification_report(labels_test, sample_prediction)))\n",
        "print(\"Confusion matrix:\\n%s\" % metrics.confusion_matrix(labels_test, sample_prediction))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tsk42BRrX2Cz"
      },
      "source": [
        "\n",
        "import seaborn as sb\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "confusion_matrix = metrics.confusion_matrix(labels_test, sample_prediction                                        )\n",
        "sb.heatmap(confusion_matrix, cmap=plt.cm.inferno)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ND0jQuTMX2Cz"
      },
      "source": [
        "**Not As Good.........**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fbcMrG4XX2Cz"
      },
      "source": [
        "# At Last\n",
        "\n",
        "I want to push all the the tweets I previously pulled from twitter into a dataframe through the SVC model ( Better Model ) and predict the sentiment.\n",
        "\n",
        "I will then compare the sentiment scores that the SVC model has predicted with that of the text_blob sentiment predictor I initially ran on the tweets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AUAn_QACX2Cz"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YNn1Rph2X2Cz"
      },
      "source": [
        "tweet_prediction = pipe.predict(df['Tweet Text'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "95HFHKGsX2C0"
      },
      "source": [
        "tweet_prediction = list(tweet_prediction)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ro018DnnX2C0"
      },
      "source": [
        "df['Tweet Prediction'] = tweet_prediction"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iNm-NxbBX2C0"
      },
      "source": [
        "df_two_preds = df[['Tweet Text', 'Sentiment', 'Tweet Prediction']]\n",
        "df_two_preds"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uq8RP3zRX2C0"
      },
      "source": [
        "c = 0\n",
        "\n",
        "for i in range(df_two_preds.shape[0]):\n",
        "    if df_two_preds['Sentiment'][i] == df_two_preds['Tweet Prediction'][i]:\n",
        "        \n",
        "        c = c + 1\n",
        "        \n",
        "print(f\"Both Models print the same value {round(c/df_two_preds.shape[0], 2)*100} percent of the time\")\n",
        "        \n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}